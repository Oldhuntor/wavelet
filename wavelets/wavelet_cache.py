import numpy as np
import pywt
import os
import multiprocessing as mp
from functools import partial
import torch
from torch.utils.data import DataLoader, TensorDataset
from typing import Optional, Tuple


def extract_cwt_features(x_single, scales, fs, trim_ratio):
    # ... (ä¿æŒä¸å˜ï¼Œç”¨äºå•æ ·æœ¬ CWT è®¡ç®—)
    time_length = x_single.shape[0]
    coefficients, _ = pywt.cwt(
        x_single, scales, 'cmor1.5-1.0', sampling_period=1/fs
    )
    amplitude = np.abs(coefficients)
    phase = np.angle(coefficients)
    cut = int(time_length * trim_ratio)
    if cut > 0:
        amplitude = amplitude[:, cut:-cut]
        phase = phase[:, cut:-cut]
    features = np.concatenate([amplitude, phase], axis=0)
    return features.flatten()

def cache_cwt_data_from_dataloader(raw_dataloader: DataLoader, scales, fs, trim_ratio, output_file, num_workers=4):
    """
    ä» DataLoader ä¸­æå–æ‰€æœ‰åŸå§‹æ•°æ®ï¼Œå¹¶è¡Œè®¡ç®— CWT ç‰¹å¾å¹¶ç¼“å­˜ã€‚
    """

    output_dir = os.path.dirname(output_file)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if os.path.exists(output_file):
        print(f"âœ… ç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨: {output_file}ã€‚è·³è¿‡è®¡ç®—ã€‚")
        return

    print("--- æ­¥éª¤ 1: è¿­ä»£ DataLoader è·å–æ‰€æœ‰åŸå§‹æ•°æ®å’Œæ ‡ç­¾ ---")
    all_raw_data = []
    all_labels = []

    # è¿­ä»£ DataLoaderï¼Œå°†å…¶ä¸­çš„æ‰€æœ‰ batch_size æ•°é‡çš„æ ·æœ¬æå–å‡ºæ¥
    for raw_batch_data, raw_batch_labels in raw_dataloader:
        # raw_batch_data å½¢çŠ¶å¯èƒ½ä¸º [B, 1, L] æˆ– [B, L]
        # è½¬æ¢ä¸º NumPyï¼Œå¹¶å±•å¹³æ‰¹æ¬¡ç»´åº¦ä»¥ä¾›å¤šè¿›ç¨‹å¤„ç†

        # å‡è®¾åŸå§‹æ•°æ®æ˜¯ [B, 1, L]ï¼Œå…ˆå»æ‰ç»´åº¦ 1
        raw_batch_data = raw_batch_data.squeeze(1)

        # è½¬æ¢ä¸º NumPy æ•°ç»„
        all_raw_data.extend(raw_batch_data.cpu().numpy())
        all_labels.extend(raw_batch_labels.cpu().numpy())

        # æ‰“å°è¿›åº¦
        if len(all_labels) % (raw_dataloader.batch_size * 10) == 0:
            print(f"å·²æå– {len(all_labels)} ä¸ªæ ·æœ¬...")

    print(f"DataLoader è¿­ä»£å®Œæ¯•ï¼Œå…±è·å– {len(all_labels)} ä¸ªæ ·æœ¬ã€‚")

    # --- æ­¥éª¤ 2: å¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®— CWT ---
    print(f"\n--- æ­¥éª¤ 2: å¼€å§‹ä½¿ç”¨ {num_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè®¡ç®— CWT ---")

    cwt_func = partial(
        extract_cwt_features,
        scales=scales,
        fs=fs,
        trim_ratio=trim_ratio
    )

    with mp.Pool(processes=num_workers) as pool:
        # ä½¿ç”¨ pool.imap_unordered å¹¶è¡Œå¤„ç†æ‰€æœ‰åŸå§‹æ•°æ®
        results_iterator = pool.imap_unordered(cwt_func, all_raw_data)

        processed_features = []

        # é€ä¸ªæ”¶é›†ç»“æœ
        for i, features in enumerate(results_iterator):
            processed_features.append(features)

            if (i + 1) % 100 == 0 or (i + 1) == len(all_labels):
                print(f"å·²å®Œæˆ {i + 1} ä¸ªæ ·æœ¬çš„ CWT è®¡ç®—...")

    print("æ‰€æœ‰æ ·æœ¬ CWT å¤„ç†å®Œæˆã€‚")

    # --- æ­¥éª¤ 3: ä¿å­˜ç»“æœ ---
    X_processed = np.stack(processed_features, axis=0)
    print(f"X_processed shape: {X_processed.shape}")
    Y_processed = np.array(all_labels)

    np.savez(output_file, X=X_processed, Y=Y_processed)
    print(f"\nğŸ‰ ç‰¹å¾å’Œæ ‡ç­¾å·²æˆåŠŸç¼“å­˜åˆ° {output_file}ã€‚")


def create_dataloader_from_npz(
        npz_file_path: str,
        batch_size: int = 32,
        shuffle: bool = True,
        mean: Optional[float] = None,
        std: Optional[float] = None
) -> Tuple[DataLoader, float, float]:
    """
    ä» npz æ–‡ä»¶åŠ è½½ CWT ç‰¹å¾ï¼Œè¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¹¶åˆ›å»º DataLoaderã€‚
    ä½¿ç”¨ torch.utils.data.TensorDataset æ›¿æ¢è‡ªå®šä¹‰ Datasetã€‚

    Args:
        npz_file_path (str): ç¼“å­˜çš„ .npz æ–‡ä»¶è·¯å¾„ã€‚
        batch_size (int): DataLoader çš„æ‰¹æ¬¡å¤§å°ã€‚
        shuffle (bool): æ˜¯å¦åœ¨æ¯ä¸ª epoch éšæœºæ‰“ä¹±æ•°æ®ã€‚
        mean (Optional[float]): ç”¨äºæ ‡å‡†åŒ–çš„å‡å€¼ã€‚å¦‚æœä¸º Noneï¼Œåˆ™è‡ªè¡Œè®¡ç®—ã€‚
        std (Optional[float]): ç”¨äºæ ‡å‡†åŒ–çš„æ ‡å‡†å·®ã€‚å¦‚æœä¸º Noneï¼Œåˆ™è‡ªè¡Œè®¡ç®—ã€‚

    Returns:
        Tuple[DataLoader, float, float]:
            (DataLoader å®ä¾‹, å®é™…ä½¿ç”¨çš„ mean, å®é™…ä½¿ç”¨çš„ std)
    """

    # 1. åŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸º Tensor
    try:
        data = np.load(npz_file_path)
        # å°†ç‰¹å¾æ•°æ®è½¬æ¢ä¸º float32 å¹¶è½¬ä¸º Tensor
        X_tensor = torch.from_numpy(data['X'].astype(np.float32))
        # å°†æ ‡ç­¾æ•°æ®è½¬æ¢ä¸º Long ç±»å‹ Tensor
        Y_tensor = torch.from_numpy(data['Y']).long()
        data.close()
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {npz_file_path}")
        raise

    print(f"æ•°æ®åŠ è½½å®Œæˆã€‚æ ·æœ¬æ€»æ•°: {X_tensor.shape[0]}ï¼Œç‰¹å¾ç»´åº¦: {X_tensor.shape[1]}")

    # 2. æ ‡å‡†åŒ– (Z-Score Normalization)
    if mean is None or std is None:
        # å¦‚æœæœªæä¾› mean å’Œ stdï¼Œåˆ™è®¡ç®—å¹¶åº”ç”¨
        print("æœªæä¾› mean/stdï¼Œæ­£åœ¨è®¡ç®—å¹¶åº”ç”¨ Z-Score æ ‡å‡†åŒ–...")

        computed_mean = X_tensor.mean()
        computed_std = X_tensor.std()

        # é˜²æ­¢é™¤é›¶
        if computed_std == 0:
            computed_std = 1.0

        X_tensor = (X_tensor - computed_mean) / computed_std

        actual_mean = computed_mean.item()
        actual_std = computed_std.item()
    else:
        # å¦‚æœæä¾›äº† mean å’Œ stdï¼Œåˆ™ä½¿ç”¨è¾“å…¥çš„å€¼
        print(f"ä½¿ç”¨æä¾›çš„ mean={mean:.4f}, std={std:.4f} è¿›è¡Œæ ‡å‡†åŒ–...")

        X_tensor = (X_tensor - mean) / std

        actual_mean = mean
        actual_std = std

    # 3. åˆ›å»º TensorDataset å’Œ DataLoader
    # ä½¿ç”¨ TensorDataset ç›´æ¥åŒ…è£…ç‰¹å¾å’Œæ ‡ç­¾ Tensor
    dataset = TensorDataset(X_tensor, Y_tensor)

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=0,  # ç‰¹å¾å·²é¢„å¤„ç†ï¼Œnum_workers è®¾ä¸º 0
    )

    print(f"DataLoader åˆ›å»ºæˆåŠŸã€‚")

    # è¿”å› DataLoader å’Œå®é™…ä½¿ç”¨çš„ mean/std
    return dataloader, actual_mean, actual_std



# --- ç¤ºä¾‹ï¼šåœ¨ä¸»æ–‡ä»¶æˆ–é¢„å¤„ç†æ–‡ä»¶ä¸­è°ƒç”¨ ---
if __name__ == '__main__':
    # å‡è®¾ä½ å·²ç»å®šä¹‰äº† MyRawDataset å’Œç›¸å…³çš„å‚æ•°
    from utils.ts_convertor import create_dataloader_from_arff
    from model.CWT import generate_adaptive_scales


    DATA_PATH = '/Users/hxh/PycharmProjects/final_thesis/Dataset/'
    DATA_NAME = 'AbnormalHeartbeat'


    TRAIN_FILE = f'{DATA_NAME}/{DATA_NAME}_TRAIN.arff'
    TEST_FILE = f'{DATA_NAME}/{DATA_NAME}_TEST.arff'

    train_path = DATA_PATH + TRAIN_FILE
    test_path = DATA_PATH + TEST_FILE


    train_loader, mean, std = create_dataloader_from_npz('/Users/hxh/PycharmProjects/final_thesis/Dataset/AbnormalHeartbeat/AbnormalHeartbeat_TEST.npz')


    train_dataloader, train_mean, train_std = create_dataloader_from_arff(
        arff_file_path=train_path, batch_size=64, shuffle=True
    )

    # æµ‹è¯•é›†ï¼šä½¿ç”¨è®­ç»ƒé›†çš„å‚æ•°è¿›è¡Œæ ‡å‡†åŒ–
    test_dataloader, _, _ = create_dataloader_from_arff(
        arff_file_path=test_path, batch_size=64, shuffle=False,
        mean=train_mean, std=train_std
    )

    # --- å®šä¹‰å‚æ•° ---
    L = 18305
    FS = 100.0
    TRIM_RATIO = 0.1
    SCALES = generate_adaptive_scales(L, num_scales=5)
    FILE_NAME = f'/{DATA_NAME}_cwt_features.npz'
    OUTPUT_FILE = DATA_PATH + DATA_NAME + FILE_NAME


    # 2. è¿è¡Œç¼“å­˜å‡½æ•°
    cache_cwt_data_from_dataloader(
        test_dataloader,
        SCALES,
        FS,
        TRIM_RATIO,
        OUTPUT_FILE,
        num_workers=10
    )

    # 3. è®­ç»ƒæ—¶ä½¿ç”¨ CachedCWTDataset æ¥è¯»å–ç¼“å­˜æ–‡ä»¶
    # from your_file import CachedCWTDataset # å‡è®¾ä½ å·²å®šä¹‰
    # train_dataset = CachedCWTDataset(OUTPUT_FILE)
    # train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)